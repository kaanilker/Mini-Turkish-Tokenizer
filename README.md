# Mini-Turkish-Tokenizer
A turkish tokenizer which has 5610 vocabulary size for little models.
**TÃ¼rkÃ§e dil modelleri iÃ§in optimize edilmiÅŸ, kompakt BPE tokenizer**

---

## ğŸ“Œ Ã–zet

Mini Turkish Tokenizer, TÃ¼rkÃ§e NLP gÃ¶revleri iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ bir BPE (Byte Pair Encoding) tokenizer'dÄ±r. CulturaX Turkish dataset'inin 735,991 dokÃ¼manÄ±ndan eÄŸitilerek, TÃ¼rkÃ§e metinleri verimli bir ÅŸekilde tokenlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

### Temel Ã–zellikler

- **Vocab Size:** 5,610 tokens (kompakt ve verimli)
- **Dil:** TÃ¼rkÃ§e (ğŸ‡¹ğŸ‡·)
- **Algoritma:** BPE (Byte Pair Encoding)
- **EÄŸitim Verisi:** CulturaX Turkish (735,991 dokÃ¼mandan)
- **Format:** HuggingFace PreTrainedTokenizerFast
- **Lisans:** GNU General Public License v2.0 (aÃ§Ä±k kaynak)

---

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§

### Kurulum

```bash
pip install transformers
```

### Temel KullanÄ±m

```python
from transformers import AutoTokenizer

# Tokenizer'Ä± yÃ¼kle
tokenizer = AutoTokenizer.from_pretrained(
    "kaanilker/mini-turkish-tokenizer"
)

# Metni tokenize et
text = "Merhaba, ben yapay zekayÄ±m!"
tokens = tokenizer.encode(text)

print(tokens)
# Output: [59, 83, 96, 86, 79, 80, ...]
```

### Decode Etme

```python
# Token'larÄ± metne geri Ã§evir
decoded = tokenizer.decode(tokens)
print(decoded)
# Output: "Merhaba, ben yapay zekayÄ±m!"
```

### Batch Processing

```python
texts = [
    "Merhaba dÃ¼nya",
    "TÃ¼rkÃ§e NLP",
    "Yapay zeka harika"
]

# Batch tokenize
encoded = tokenizer(
    texts,
    padding=True,
    truncation=True,
    max_length=100,
    return_tensors="pt"
)

print(encoded['input_ids'].shape)
# Output: torch.Size([3, 100])
```

---

## ğŸ“Š Teknik Detaylar

### Special Tokens

| Token | ID | AÃ§Ä±klama |
|---|---|---|
| `<pad>` | 0 | Padding (doldurma) |
| `<unk>` | 1 | Unknown (bilinmeyen) |
| `<bos>` | 2 | Beginning of Sequence (baÅŸlangÄ±Ã§) |
| `<eos>` | 3 | End of Sequence (bitiÅŸ) |

### EÄŸitim KonfigÃ¼rasyonu

```python
vocab_size = 5610
min_frequency = 2
algorithm = "BPE"
pre_tokenizer = "Whitespace + Punctuation"
training_data = "CulturaX Turkish (735,991 documents)"
train_test_split = "90/10"
```

### Tokenizasyon Ã–zellikleri

- **Ortalama Token SayÄ±sÄ±:** 8-12 token per sentence
- **Coverage (CulturaX):** ~98.5%
- **Encoding HÄ±zÄ±:** ~10,000 token/sec
- **Bellek Footprint:** 5-10 MB

---

## ğŸ’» Ä°leri KullanÄ±m

### Attention Mask Ä°le

```python
from transformers import AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained(
    "kaanilker/mini-turkish-tokenizer"
)

text = "KÄ±sa"
encoded = tokenizer(
    text,
    padding="max_length",
    max_length=10,
    return_tensors="pt"
)

print(encoded['input_ids'])
# [1234, 0, 0, 0, 0, 0, 0, 0, 0, 0]

print(encoded['attention_mask'])
# [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
```

### Model EÄŸitmede KullanÄ±m

```python
from transformers import TrainingArguments, Trainer
from transformers import LlamaForCausalLM, AutoTokenizer

model = LlamaForCausalLM.from_pretrained("your-model")
tokenizer = AutoTokenizer.from_pretrained(
    "kaanilker/mini-turkish-tokenizer"
)

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=32,
    learning_rate=5e-4,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    tokenizer=tokenizer,
)

trainer.train()
```

### Fine-tuning Ä°Ã§in

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained(
    "kaanilker/mini-turkish-tokenizer"
)

model = AutoModelForSequenceClassification.from_pretrained(
    "dbmdz/bert-base-turkish-cased"
)

# TÃ¼rkÃ§e metinleri tokenize et
inputs = tokenizer(
    ["Bu Ã§ok gÃ¼zel!", "Berbat!"],
    truncation=True,
    max_length=512,
    return_tensors="pt"
)

outputs = model(**inputs)
```

---

## ğŸ¯ KullanÄ±m SenaryolarÄ±

### 1. TÃ¼rkÃ§e Metin SÄ±nÄ±flandÄ±rmasÄ±
```python
from transformers import pipeline

classifier = pipeline(
    "text-classification",
    model="dbmdz/bert-base-turkish-cased",
    tokenizer=tokenizer
)

result = classifier("Bu Ã¼rÃ¼n harika!")
print(result)
```

### 2. TÃ¼rkÃ§e Metin Ãœretimi
```python
from transformers import pipeline

generator = pipeline(
    "text-generation",
    model="your-turkish-llm",
    tokenizer=tokenizer
)

generated = generator("TÃ¼rkiye'nin baÅŸkenti", max_length=50)
print(generated)
```

### 3. TÃ¼rkÃ§e Soru-Cevap
```python
from transformers import pipeline

qa = pipeline(
    "question-answering",
    model="your-qa-model",
    tokenizer=tokenizer
)

result = qa(
    question="TÃ¼rkiye'nin baÅŸkenti neresidir?",
    context="TÃ¼rkiye'nin baÅŸkenti Ankara'dÄ±r."
)
print(result)
```

### 4. TÃ¼rkÃ§e Sentiment Analizi
```python
tokenizer = AutoTokenizer.from_pretrained(
    "kaanilker/mini-turkish-tokenizer"
)

texts = [
    "Ã‡ok gÃ¼zel bir Ã¼rÃ¼n!",
    "Berbat kalite",
    "Fena deÄŸil"
]

for text in texts:
    tokens = tokenizer.encode(text)
    print(f"{text} â†’ {len(tokens)} tokens")
```

---

## ğŸ“‹ Teknik Ã–zellikler

### Vocab DaÄŸÄ±lÄ±mÄ±

```
Toplam Tokens: 5,610

Kategori DaÄŸÄ±lÄ±mÄ±:
â”œâ”€â”€ TÃ¼rkÃ§e Kelimeler: ~3,366 (60%)
â”œâ”€â”€ Subword Pieces: ~2,200 (39%)
â”œâ”€â”€ Special Tokens: 4 (1%)
â””â”€â”€ DiÄŸer: ~40 (1%)
```

### EÄŸitim Verileri

- **Dataset:** CulturaX Turkish
- **Toplam DokÃ¼mandan:** 735,991
- **Toplam Token:** 500M
- **Train/Val Split:** 90/10
- **Min Frequency:** 2 (en az 2 kez gÃ¶rÃ¼lmÃ¼ÅŸ kelimeler)

---

## ğŸ”§ Kurulum & BaÄŸÄ±mlÄ±lÄ±klar

### Gerekli Paketler

```bash
# Temel
pip install transformers>=4.30.0
pip install datasets>=2.0.0

# Ä°steÄŸe baÄŸlÄ± (Ã¶rnek kodlar iÃ§in)
pip install torch>=1.9.0
pip install pytorch-lightning>=1.5.0
```

### Versiyonlar

```
Python: 3.8+
Transformers: 4.30+
Datasets: 2.0+
Torch: 1.9+
```

---

## ğŸ“š Ã–rnekler

### Ã–rnek 1: Temel Tokenizasyon

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    "kaanilker/mini-turkish-tokenizer"
)

# Basit cÃ¼mle
text = "GÃ¼naydÄ±n, nasÄ±lsÄ±n?"
tokens = tokenizer.encode(text)
print(f"Tokens: {tokens}")
print(f"Token SayÄ±sÄ±: {len(tokens)}")

# Decode
decoded = tokenizer.decode(tokens)
print(f"Decoded: {decoded}")
```

**Output:**
```
Tokens: [59, 83, 96, ...]
Token SayÄ±sÄ±: 5
Decoded: GÃ¼naydÄ±n, nasÄ±lsÄ±n?
```

### Ã–rnek 2: Batch Tokenizasyon

```python
texts = [
    "Merhaba dÃ¼nya",
    "TÃ¼rkÃ§e NLP harika",
    "AÃ§Ä±k kaynak yazÄ±lÄ±m"
]

batch = tokenizer(
    texts,
    padding=True,
    truncation=True,
    max_length=20,
    return_tensors="pt"
)

print(batch['input_ids'].shape)
# torch.Size([3, 20])
```

### Ã–rnek 3: Dilbilimsel Analiz

```python
# Kelime parÃ§alanmasÄ±
text = "Ãœniversitelerimizde"
tokens = tokenizer.tokenize(text)
print(f"ParÃ§alar: {tokens}")
# ParÃ§alar: ['Ãœniversite', 'leri', 'mizde']

# Token ID'leri
ids = tokenizer.convert_tokens_to_ids(tokens)
print(f"IDs: {ids}")
```

---

## âš ï¸ SÄ±nÄ±rlamalar

### BilinÃ§li KÄ±sÄ±tlamalar

1. **Vocab Size:** 5,610 (kÃ¼Ã§Ã¼k ama verimli)
   - âœ… HÄ±zlÄ± tokenizasyon
   - âŒ Nadir kelimeleri parÃ§alayabilir

2. **TÃ¼rkÃ§eye Ã–zel:** Sadece TÃ¼rkÃ§e iÃ§in optimize
   - âœ… TÃ¼rkÃ§e iÃ§in en iyi
   - âŒ Ä°ngilizce vb. dillerle sorun olabilir

3. **CulturaX Bias:** Belirli alanlara biased olabilir
   - âœ… Haber, sosyal medya vb. iyi
   - âŒ Teknik jargon eksik olabilir

### Ã‡Ã¶zÃ¼mler

```python
# EÄŸer UNK token Ã§ok gÃ¶rÃ¼rsen:
# 1. Vocab'i bÃ¼yÃ¼t
# 2. FarklÄ± dataset kullan
# 3. Subword parÃ§alamayÄ± artÄ±r
```

---

## ğŸ”„ GÃ¼ncelleme 


### GÃ¼ncelleme NasÄ±l YapÄ±lÄ±r?

```bash
# En son sÃ¼rÃ¼mÃ¼ kur
pip install --upgrade transformers

# Tokenizer'Ä± gÃ¼ncelle
tokenizer = AutoTokenizer.from_pretrained(
    "kaanilker/mini-turkish-tokenizer",
    revision="main"
)
```

---

## ğŸ“– Kaynaklar

### Teorik Kaynaklar

- [Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)
- [HuggingFace Tokenizers](https://huggingface.co/docs/tokenizers/)
- [Transformers Documentation](https://huggingface.co/docs/transformers/)

### Benzer Projeler

- [Turkish BERT](https://github.com/dbmdz/bert-models)
- [Turkish GPT-2](https://huggingface.co/gpt2-turkish)
- [CulturaX Dataset](https://huggingface.co/datasets/uonlp/CulturaX)

---

## ğŸ¤ KatkÄ± ve Ä°letiÅŸim

### KatkÄ± Yapmak

```bash
git clone https://huggingface.co/kaanilker/mini-turkish-tokenizer
cd mini-turkish-tokenizer

# DeÄŸiÅŸiklik yap
git add .
git commit -m "Improvement: [aÃ§Ä±klama]"
git push
```

### Sorun Bildirmek

Email'e posta at:
- Email: kaanilkernacar2010@gmail.com

---

## ğŸ“ Sitasyon

Bu tokenizer'Ä± bilimsel Ã§alÄ±ÅŸmalarda kullanÄ±yorsan, lÃ¼tfen ÅŸunu alÄ±ntÄ± yap:

```bibtex
@software{mini_turkish_tokenizer,
  title = {Mini Turkish Tokenizer},
  author = {[Kaan Ä°lker Nacar]},
  year = {2025},
  url = {https://huggingface.co/your-username/mini-turkish-tokenizer},
  license = {GPL-2.0}
}
```

### APA Format

[Kaan Ä°lker Nacar]. (2025). Mini Turkish Tokenizer. HuggingFace Hub. Retrieved from https://huggingface.co/kaanilker/mini-turkish-tokenizer

---

## ğŸ“œ Lisans

**GNU General Public License v2.0**

Bu tokenizer aÃ§Ä±k kaynak yazÄ±lÄ±mdÄ±r. Ã–zgÃ¼rce:
- âœ… Kullanabilir
- âœ… DeÄŸiÅŸtirebilir
- âœ… DaÄŸÄ±tabilir
- âŒ AMA: TÃ¼rev eserler de GPL v2.0 olmalÄ±

---

---

## âœ¨ TeÅŸekkÃ¼rler

- **CulturaX Dataset** - TÃ¼rkÃ§e veri saÄŸlayan uonlp
- **HuggingFace** - Tokenizer kÃ¼tÃ¼phanesi
- **Transformers** - NLP framework
- **AÃ§Ä±k Kaynak TopluluÄŸu** - Destekler ve geri bildirim

---

## ğŸ“ Ä°letiÅŸim

- ğŸ“§ Email: kaanilkernacar2010@gmail.com
- ğŸ”— GitHub: https://github.com/kaanilker
- ğŸ¤— HuggingFace: https://huggingface.co/kaanilker
---

**Made with â¤ï¸ for Turkish NLP Community**

*Son gÃ¼ncelleme: AralÄ±k 2025*
